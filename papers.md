# aisys2023





# Paper list

## OSDI

### 2018

Ray: A Distributed Framework for Emerging AI Applications
Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang, Melih Elibol, Zongheng Yang, William Paul, Michael I. Jordan, and Ion Stoica, UC Berkeley

TVM: An Automated End-to-End Optimizing Compiler for Deep Learning
Tianqi Chen and Thierry Moreau, University of Washington; Ziheng Jiang, University of Washington, AWS; Lianmin Zheng, Shanghai Jiao Tong University; Eddie Yan, Haichen Shen, and Meghan Cowan, University of Washington; Leyuan Wang, UC Davis, AWS; Yuwei Hu, Cornell; Luis Ceze, Carlos Guestrin, and Arvind Krishnamurthy, University of Washington

Gandiva: Introspective Cluster Scheduling for Deep Learning
Wencong Xiao, Beihang University & Microsoft Research; Romil Bhardwaj, Ramachandran Ramjee, Muthian Sivathanu, and Nipun Kwatra, Microsoft Research; Zhenhua Han, The University of Hong Kong & Microsoft Research; Pratyush Patel, Microsoft Research; Xuan Peng, Huazhong University of Science and Technology & Microsoft Research; Hanyu Zhao, Peking University & Microsoft Research; Quanlu Zhang, Fan Yang, and Lidong Zhou, Microsoft Research

PRETZEL: Opening the Black Box of Machine Learning Prediction Serving Systems
Yunseong Lee, Seoul National University; Alberto Scolari, Politecnico di Milano; Byung-Gon Chun, Seoul National University; Marco Domenico Santambrogio, Politecnico di Milano; Markus Weimer and Matteo Interlandi, Microsoft

### 2020

Serving DNNs like Clockwork: Performance Predictability from the Bottom Up
Arpan Gujarati, Max Planck Institute for Software Systems; Reza Karimi, Emory University; Safya Alzayat, Wei Hao, and Antoine Kaufmann, Max Planck Institute for Software Systems; Ymir Vigfusson, Emory University; Jonathan Mace, Max Planck Institute for Software Systems

A Unified Architecture for Accelerating Distributed DNN Training in Heterogeneous GPU/CPU Clusters
Yimin Jiang, Tsinghua University and ByteDance; Yibo Zhu, ByteDance; Chang Lan, Google; Bairen Yi, ByteDance; Yong Cui, Tsinghua University; Chuanxiong Guo, ByteDance

Heterogeneity-Aware Cluster Scheduling Policies for Deep Learning Workloads
Deepak Narayanan and Keshav Santhanam, Stanford University and Microsoft Research; Fiodar Kazhamiaka, Stanford University; Amar Phanishayee, Microsoft Research; Matei Zaharia, Stanford University

PipeSwitch: Fast Pipelined Context Switching for Deep Learning Applications
Zhihao Bai and Zhen Zhang, Johns Hopkins University; Yibo Zhu, ByteDance Inc.; Xin Jin, Johns Hopkins University

HiveD: Sharing a GPU Cluster for Deep Learning with Guarantees
Hanyu Zhao, Peking University and Microsoft; Zhenhua Han, The University of Hong Kong and Microsoft; Zhi Yang, Peking University; Quanlu Zhang, Fan Yang, Lidong Zhou, and Mao Yang, Microsoft; Francis C.M. Lau, The University of Hong Kong; Yuqi Wang, Yifan Xiong, and Bin Wang, Microsoft

AntMan: Dynamic Scaling on GPU Clusters for Deep Learning
Wencong Xiao, Shiru Ren, Yong Li, Yang Zhang, Pengyang Hou, Zhi Li, Yihui Feng, Wei Lin, and Yangqing Jia, Alibaba Group


Twine: A Unified Cluster Management System for Shared Infrastructure
Chunqiang Tang, Kenny Yu, Kaushik Veeraraghavan, Jonathan Kaldor, Scott Michelson, Thawan Kooburat, Aravind Anbudurai, Matthew Clark, Kabir Gogia, Long Cheng, Ben Christensen, Alex Gartrell, Maxim Khutornenko, Sachin Kulkarni, Marcin Pawlowski, Tuomas Pelkonen, Andre Rodrigues, Rounak Tibrewal, Vaishnavi Venkatesan, and Peter Zhang, Facebook Inc.

Building Scalable and Flexible Cluster Managers Using Declarative Programming
Lalith Suresh, VMware; João Loff, IST (ULisboa) / INESC-ID; Faria Kalim, UIUC; Sangeetha Abdu Jyothi, UC Irvine and VMware; Nina Narodytska, Leonid Ryzhyk, Sahan Gamage, Brian Oki, Pranshu Jain, and Michael Gasch, VMware


Ansor: Generating High-Performance Tensor Programs for Deep Learning
Lianmin Zheng, UC Berkeley; Chengfan Jia, Minmin Sun, and Zhao Wu, Alibaba Group; Cody Hao Yu, Amazon Web Services; Ameer Haj-Ali, UC Berkeley; Yida Wang, Amazon Web Services; Jun Yang, Alibaba Group; Danyang Zhuo, UC Berkeley and Duke University; Koushik Sen, Joseph E. Gonzalez, and Ion Stoica, UC Berkeley


Rammer: Enabling Holistic Deep Learning Compiler Optimizations with rTasks
Lingxiao Ma, Peking University and Microsoft Research; Zhiqiang Xie, ShanghaiTech University and Microsoft Research; Zhi Yang, Peking University; Jilong Xue, Youshan Miao, Wei Cui, Wenxiang Hu, Fan Yang, Lintao Zhang, and Lidong Zhou, Microsoft Research


A Tensor Compiler for Unified Machine Learning Prediction Serving
Supun Nakandala, UC San Diego; Karla Saur, Microsoft; Gyeong-In Yu, Seoul National University; Konstantinos Karanasos, Carlo Curino, Markus Weimer, and Matteo Interlandi, Microsoft


Retiarii: A Deep Learning Exploratory-Training Framework
Quanlu Zhang, Zhenhua Han, Fan Yang, Yuge Zhang, Zhe Liu, Mao Yang, and Lidong Zhou, Microsoft Research

KungFu: Making Training in Distributed Machine Learning Adaptive
Luo Mai, Guo Li, Marcel Wagenländer, Konstantinos Fertakis, Andrei-Octavian Brabete, and Peter Pietzuch, Imperial College London

### 2021

Pollux: Co-adaptive Cluster Scheduling for Goodput-Optimized Deep Learning
Aurick Qiao, Petuum, Inc. and Carnegie Mellon University; Sang Keun Choe and Suhas Jayaram Subramanya, Carnegie Mellon University; Willie Neiswanger, Petuum, Inc. and Carnegie Mellon University; Qirong Ho, Petuum, Inc.; Hao Zhang, Petuum, Inc. and UC Berkeley; Gregory R. Ganger, Carnegie Mellon University; Eric P. Xing, MBZUAI, Petuum, Inc., and Carnegie Mellon University

Oort: Efficient Federated Learning via Guided Participant Selection
Fan Lai, Xiangfeng Zhu, Harsha V. Madhyastha, and Mosharaf Chowdhury, University of Michigan

PET: Optimizing Tensor Programs with Partially Equivalent Transformations and Automated Corrections
Haojie Wang, Jidong Zhai, Mingyu Gao, Zixuan Ma, Shizhi Tang, and Liyan Zheng, Tsinghua University; Yuanzhi Li, Carnegie Mellon University; Kaiyuan Rong and Yuanyong Chen, Tsinghua University; Zhihao Jia, Carnegie Mellon University and Facebook

Dorylus: Affordable, Scalable, and Accurate GNN Training with Distributed CPU Servers and Serverless Threads
John Thorpe, Yifan Qiao, Jonathan Eyolfson, and Shen Teng, UCLA; Guanzhou Hu, UCLA and University of Wisconsin, Madison; Zhihao Jia, CMU; Jinliang Wei, Google Brain; Keval Vora, Simon Fraser; Ravi Netravali, Princeton University; Miryung Kim and Guoqing Harry Xu, UCLA

GNNAdvisor: An Adaptive and Efficient Runtime System for GNN Acceleration on GPUs
Yuke Wang, Boyuan Feng, Gushu Li, Shuangchen Li, Lei Deng, Yuan Xie, and Yufei Ding, University of California, Santa Barbara

Marius: Learning Massive Graph Embeddings on a Single Machine
Jason Mohoney and Roger Waleffe, University of Wisconsin–Madison; Henry Xu, University of Maryland, College Park; Theodoros Rekatsinas and Shivaram Venkataraman, University of Wisconsin–Madison


P3: Distributed Deep Graph Learning at Scale
Swapnil Gandhi and Anand Padmanabha Iyer, Microsoft Research

### 2022

SparTA: Deep-Learning Model Sparsity via Tensor-with-Sparsity-Attribute
Ningxin Zheng, Microsoft Research; Bin Lin, Microsoft Research and Tsinghua University; Quanlu Zhang, Lingxiao Ma, Yuqing Yang, Fan Yang, Yang Wang, Mao Yang, and Lidong Zhou, Microsoft Research

ROLLER: Fast and Efficient Tensor Compilation for Deep Learning
Hongyu Zhu, University of Toronto and Microsoft Research; Ruofan Wu, Renmin University of China and Microsoft Research; Yijia Diao, Shanghai Jiao Tong University and Microsoft Research; Shanbin Ke, UCSD and Microsoft Research; Haoyu Li, Columbia University and Microsoft Research; Chen Zhang, Tsinghua University and Microsoft Research; Jilong Xue, Lingxiao Ma, Yuqing Xia, Wei Cui, Fan Yang, Mao Yang, and Lidong Zhou, Microsoft Research; Asaf Cidon, Columbia University; Gennady Pekhimenko, University of Toronto

Walle: An End-to-End, General-Purpose, and Large-Scale Production System for Device-Cloud Collaborative Machine Learning
Chengfei Lv, Zhejiang University and Alibaba Group; Chaoyue Niu, Shanghai Jiao Tong University and Alibaba Group; Renjie Gu, Xiaotang Jiang, Zhaode Wang, Bin Liu, Ziqi Wu, Qiulin Yao, Congyu Huang, Panos Huang, Tao Huang, Hui Shu, Jinde Song, Bin Zou, Peng Lan, and Guohuan Xu, Alibaba Group; Fei Wu, Zhejiang University; Shaojie Tang, University of Texas at Dallas; Fan Wu and Guihai Chen, Shanghai Jiao Tong University

Unity: Accelerating DNN Training Through Joint Optimization of Algebraic Transformations and Parallelization
Colin Unger, Stanford University; Zhihao Jia, Carnegie Mellon University and Meta; Wei Wu, Los Alamos National Laboratory and NVIDIA; Sina Lin, Microsoft; Mandeep Baines and Carlos Efrain Quintero Narvaez, Meta; Vinay Ramakrishnaiah, Nirmal Prajapati, Pat McCormick, and Jamaludin Mohd-Yusof, Los Alamos National Laboratory; Xi Luo, SLAC National Accelerator Laboratory; Dheevatsa Mudigere, Jongsoo Park, and Misha Smelyanskiy, Meta; Alex Aiken, Stanford University

Orca: A Distributed Serving System for Transformer-Based Generative Models
Gyeong-In Yu and Joo Seong Jeong, Seoul National University; Geon-Woo Kim, FriendliAI and Seoul National University; Soojeong Kim, FriendliAI; Byung-Gon Chun, FriendliAI and Seoul National University

Microsecond-scale Preemption for Concurrent GPU-accelerated DNN Inferences
Mingcong Han, Institute of Parallel and Distributed Systems, SEIEE, Shanghai Jiao Tong University; Shanghai AI Laboratory; Hanze Zhang, Institute of Parallel and Distributed Systems, SEIEE, Shanghai Jiao Tong University; MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, China; Rong Chen, Institute of Parallel and Distributed Systems, SEIEE, Shanghai Jiao Tong University; Shanghai AI Laboratory; Haibo Chen, Institute of Parallel and Distributed Systems, SEIEE, Shanghai Jiao Tong University; Engineering Research Center for Domain-specific Operating Systems, Ministry of Education, China


Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning
Lianmin Zheng, Zhuohan Li, and Hao Zhang, UC Berkeley; Yonghao Zhuang, Shanghai Jiao Tong University; Zhifeng Chen and Yanping Huang, Google; Yida Wang, Amazon Web Services; Yuanzhong Xu, Google; Danyang Zhuo, Duke University; Eric P. Xing, MBZUAI and Carnegie Mellon University; Joseph E. Gonzalez and Ion Stoica, UC Berkeley

Looking Beyond GPUs for DNN Scheduling on Multi-Tenant Clusters
Jayashree Mohan, Amar Phanishayee, and Janardhan Kulkarni, Microsoft Research; Vijay Chidambaram, University of Texas at Austin and VMware Research

Ekko: A Large-Scale Deep Learning Recommender System with Low-Latency Model Update
Chijun Sima, Tencent; Yao Fu and Man-Kit Sit, The University of Edinburgh; Liyi Guo, Xuri Gong, Feng Lin, Junyu Wu, Yongsheng Li, and Haidong Rong, Tencent; Pierre-Louis Aublin, IIJ research laboratory; Luo Mai, The University of Edinburgh


Efficient and Scalable Graph Pattern Mining on GPUs
Xuhao Chen and Arvind, MIT CSAIL


## SOSP

### 2019

PipeDream: Generalized Pipeline Parallelism for DNN Training
Deepak Narayanan (Stanford University), Aaron Harlap (Carnegie Mellon University), Amar Phanishayee (Microsoft Research), Vivek Seshadri (Microsoft Research), Nikhil R. Devanur (Microsoft Research), Gregory R. Ganger (CMU), Phillip B. Gibbons (Carnegie Mellon University), Matei Zaharia (Stanford University)


A Generic Communication Scheduler for Distributed DNN Training Acceleration
Yanghua Peng (The University of Hong Kong), Yibo Zhu (ByteDance Inc.), Yangrui Chen (The University of Hong Kong), Yixin Bao (The University of Hong Kong), Bairen Yi (ByteDance Inc.), Chang Lan (ByteDance Inc.), Chuan Wu (The University of Hong Kong), Chuanxiong Guo (ByteDance Inc.)


Parity Models: Erasure-Coded Resilience for Prediction Serving Systems
Jack Kosaian (Carnegie Mellon University), K. V. Rashmi (Carnegie Mellon University), Shivaram Venkataraman (University of Wisconsin-Madison)


TASO: Optimizing Deep Learning Computation with Automated Generation of Graph Substitutions
Zhihao Jia (Stanford University), Oded Padon (Stanford University), James Thomas (Stanford University), Todd Warszawski (Stanford University), Matei Zaharia (Stanford University), Alex Aiken (Stanford Univeristy)

Nexus: A GPU Cluster Engine for Accelerating DNN-Based Video Analysis
Haichen Shen (Amazon Web Services), Lequn Chen (University of Washington), Yuchen Jin (University of Washington), Liangyu Zhao (University of Washington), Bingyu Kong (Shanghai Jiao Tong University), Matthai Philipose (Microsoft Research), Arvind Krishnamurthy (University of Washington), Ravi Sundaram (Northeastern University)

### 2021

Gradient Compression Supercharged High-Performance Data Parallel DNN Training
Youhui Bai (University of Science and Technology of China), Cheng Li (University of Science and Technology of China), Quan Zhou (University of Science and Technology of China), Jun Yi (University of Nevada at Reno), Ping Gong (University of Science and Technology of China), Feng Yan (University of Nevada at Reno), Ruichuan Chen (Nokia Bell Labs), Yinlong Xu (University of Science and Technology of China)


## NSDI



### 2019

JANUS. JANUS: Fast and Flexible Deep Learning via Symbolic Graph Execution of Imperative Programs. Eunji Jeong, Sungwoo Cho, Gyeong-In Yu, Joo Seong Jeong, Dong-Jin Shin, Byung-Gon Chun. NSDI 2019.


BLAS-on-flash: An Efficient Alternative for Large Scale ML Training and Inference?
Suhas Jayaram Subramanya and Harsha Vardhan Simhadri, Microsoft Research India; Srajan Garg, IIT Bombay; Anil Kag and Venkatesh Balasubramanian, Microsoft Research India

Tiresias: A GPU Cluster Manager for Distributed Deep Learning
Juncheng Gu, Mosharaf Chowdhury, and Kang G. Shin, University of Michigan, Ann Arbor; Yibo Zhu, Microsoft and Bytedance; Myeongjae Jeon, Microsoft and UNIST; Junjie Qian, Microsoft; Hongqiang Liu, Alibaba; Chuanxiong Guo, Bytedance

### 2020
Themis: Fair and Efficient GPU Cluster Scheduling
Kshiteej Mahajan, Arjun Balasubramanian, Arjun Singhvi, Shivaram Venkataraman, and Aditya Akella, University of Wisconsin-Madison; Amar Phanishayee, Microsoft Research; Shuchi Chawla, University of Wisconsin-Madison

### 2021
Mistify: Automating DNN Model Porting for On-Device Inference at the Edge
Peizhen Guo, Bo Hu, and Wenjun Hu, Yale University

Elastic Resource Sharing for Distributed Deep Learning
Changho Hwang and Taehyun Kim, KAIST; Sunghyun Kim, MIT; Jinwoo Shin and KyoungSoo Park, KAIST

ATP: In-network Aggregation for Multi-tenant Learning
ChonLam Lao, Tsinghua University; Yanfang Le and Kshiteej Mahajan, University of Wisconsin-Madison; Yixi Chen and Wenfei Wu, Tsinghua University; Aditya Akella and Michael Swift, University of Wisconsin-Madison

Scaling Distributed Machine Learning with In-Network Aggregation
Amedeo Sapio, Marco Canini, and Chen-Yu Ho, KAUST; Jacob Nelson, Microsoft; Panos Kalnis, KAUST; Changhoon Kim, Barefoot Networks; Arvind Krishnamurthy, University of Washington; Masoud Moshref, Barefoot Networks; Dan Ports, Microsoft; Peter Richtarik, KAUST

### 2022

Check-N-Run: a Checkpointing System for Training Deep Learning Recommendation Models
Assaf Eisenman, Kiran Kumar Matam, Steven Ingram, Dheevatsa Mudigere, Raghuraman Krishnamoorthi, Krishnakumar Nair, and Misha Smelyanskiy, Facebook; Murali Annavaram, Facebook and USCa


MLaaS in the Wild: Workload Analysis and Scheduling in Large-Scale Heterogeneous GPU Clusters
Qizhen Weng, Hong Kong University of Science and Technology and Alibaba Group; Wencong Xiao, Alibaba Group; Yinghao Yu, Alibaba Group and Hong Kong University of Science and Technology; Wei Wang, Hong Kong University of Science and Technology; Cheng Wang, Jian He, Yong Li, Liping Zhang, Wei Lin, and Yu Ding, Alibaba Group

Accelerating Collective Communication in Data Parallel Training across Deep Learning Frameworks
Joshua Romero, NVIDIA, Inc.; Junqi Yin, Nouamane Laanait, Bing Xie, and M. Todd Young, Oak Ridge National Laboratory; Sean Treichler, NVIDIA, Inc.; Vitalii Starchenko and Albina Borisevich, Oak Ridge National Laboratory; Alex Sergeev, Carbon Robotics; Michael Matheson, Oak Ridge National Laboratory

Cocktail: A Multidimensional Optimization for Model Serving in Cloud
Jashwant Raj Gunasekaran, Cyan Subhra Mishra, Prashanth Thinakaran, Bikash Sharma, Mahmut Taylan Kandemir, and Chita R. Das, The Pennsylvania State University



### 2023

Transparent GPU Sharing in Container Clouds for Deep Learning Workloads
Bingyang Wu and Zili Zhang, Peking University; Zhihao Bai, Johns Hopkins University; Xuanzhe Liu and Xin Jin, Peking University

ARK: GPU-driven Code Execution for Distributed Deep Learning
Changho Hwang, Microsoft Research; KyoungSoo Park, KAIST; Ran Shu, Xinyuan Qu, Peng Cheng, and Yongqiang Xiong, Microsoft Research

BGL: GPU-Efficient GNN Training by Optimizing Graph Data I/O and Preprocessing
Tianfeng Liu, Tsinghua University, Zhongguancun Laboratory, ByteDance; Yangrui Chen, The University of Hong Kong, ByteDance; Dan Li, Tsinghua University, Zhongguancun Laboratory; Chuan Wu, The University of Hong Kong; Yibo Zhu, Jun He, and Yanghua Peng, ByteDance; Hongzheng Chen, ByteDance, Cornell University; Hongzhi Chen and Chuanxiong Guo, ByteDance

Zeus: Understanding and Optimizing GPU Energy Consumption of DNN Training
Jie You, Jae-Won Chung, and Mosharaf Chowdhury, University of Michigan

Bamboo: Making Preemptible Instances Resilient for Affordable Training of Large DNNs
John Thorpe, Pengzhan Zhao, Jonathan Eyolfson, and Yifan Qiao, UCLA; Zhihao Jia, CMU; Minjia Zhang, Microsoft Research; Ravi Netravali, Princeton University; Guoqing Harry Xu, UCLA

Shockwave: Fair and Efficient Cluster Scheduling for Dynamic Adaptation in Machine Learning
Pengfei Zheng and Rui Pan, University of Wisconsin-Madison; Tarannum Khan, The University of Texas at Austin; Shivaram Venkataraman, University of Wisconsin-Madison; Aditya Akella, The University of Texas at Austin

TopoOpt: Co-optimizing Network Topology and Parallelization Strategy for Distributed Training Jobs
Weiyang Wang, Moein Khazraee, Zhizhen Zhong, and Manya Ghobadi, Massachusetts Institute of Technology; Zhihao Jia, Meta and CMU; Dheevatsa Mudigere and Ying Zhang, Meta; Anthony Kewitsch, Telescent

ModelKeeper: Accelerating DNN Training via Automated Training Warmup
Fan Lai, Yinwei Dai, Harsha V. Madhyastha, and Mosharaf Chowdhury, University of Michigan

SHEPHERD: Serving DNNs in the Wild
Hong Zhang, University of Waterloo; Yupeng Tang and Anurag Khandelwal, Yale University; Ion Stoica, UC Berkeley

Better Together: Jointly Optimizing ML Collective Scheduling and Execution Planning using SYNDICATE
Kshiteej Mahajan, University of Wisconsin - Madison; Ching-Hsiang Chu and Srinivas Sridharan, Facebook; Aditya Akella, UT Austin

On Modular Learning of Distributed Systems for Predicting End-to-End Latency
Chieh-Jan Mike Liang, Microsoft Research; Zilin Fang, Carnegie Mellon University; Yuqing Xie, Tsinghua University; Fan Yang, Microsoft Research; Zhao Lucis Li, University of Science and Technology of China; Li Lyna Zhang, Mao Yang, and Lidong Zhou, Microsoft Research

SelfTune: Tuning Cluster Managers
Ajaykrishna Karthikeyan and Nagarajan Natarajan, Microsoft Research; Gagan Somashekar, Stony Brook University; Lei Zhao, Microsoft; Ranjita Bhagwan, Microsoft Research; Rodrigo Fonseca, Tatiana Racheva, and Yogesh Bansal, Microsoft





## EuroSys

### 2018

Optimus: An Efficient Dynamic Resource Scheduler for Deep Learning Clusters | Yanghua Peng, Yixin Bao, Yangrui Chen, Chuan Wu (University of Hong Kong), and Chuanxiong Guo (Bytedance Inc.)

Dynamic Control Flow in Large-Scale Machine Learning | Yuan Yu (Microsoft), Martin Abadi, Paul Barham, Eugene Brevdo, Mike Burrows, Andy Davis, Jeff Dean, Sanjay Ghemawat (Google), Tim Harley (DeepMind), Peter Hawkins, Michael Isard (Google), Manjunath Kudlur (Cerebras), Rajat Monga, Derek Murray, and Xiaoqiang Zheng (Google)

Improving the Expressiveness of Deep Learning Frameworks with Recursion | Eunji Jeong, Joo Seong Jeong, Soojeong Kim, Gyeong-In Yu, and Byung-Gon Chun (Seoul National University)


Low Latency RNN Inference with Cellular Batching | Pin Gao (Tsinghua University), Lingfan Yu (New York University), Yongwei Wu (Tsinghua University), and Jinyang Li (New York University)

### 2019

Supporting Very Large Models using Dataflow Graph Partitioning
Minjie Wang, Chien-chin Huang, and Jinyang Li (NYU)

GRNN: Low-Latency and Scalable RNN Inference on GPUs
Connor Holmes and Daniel Mawhirter (Colorado School of Mines); Yuxiong He (Microsoft Business AI and Research); Feng Yan (University of Nevada, Reno); Bo Wu (Colorado School of Mines)

Automating Dependence-Aware Parallelization of Machine Learning Training on Distributed Shared Memory
Jinliang Wei (Carnegie Mellon University); Garth Gibson (Vector Institute, Carnegie Mellon University, University of Toronto); Philip Gibbons (Carnegie Mellon University); Eric Xing (Petuum, Carnegie Mellon University)

Parallax: Sparsity-aware Data Parallel Training of Deep Neural Networks
Soojeong Kim, Gyeong-In Yu, Hojin Park, Sungwoo Cho, Eunji Jeong, Hyeonmin Ha, Sanha Lee, Joo Seong Jeong, and Byung-Gon Chun (Seoul National University)

### 2020

Borg: the Next Generation
Muhammad Tirmazi (Harvard University), Adam Barker (Google and University of St Andrews), Nan Deng, Md Ehtesam Haque, Zhijing Gene Qin, Steven Hand (Google), Mor Harchol-Balter (Carnegie Mellon University), John Wilkes (Google)

AlloX: Compute Allocation in Hybrid Clusters
Tan N. Le (SUNY Korea, Stony Brook University), Xiao Sun (Stony Brook University), Mosharaf Chowdhury (University of Michigan), Zhenhua Liu (Stony Brook University)


Balancing Efficiency and Fairness in Heterogeneous GPU Clusters for Deep Learning
Shubham Chaudhary, Ramachandran Ramjee, Muthian Sivathanu, Nipun Kwatra, Srinidhi Viswanatha (Microsoft Research India)

### 2021

FlexGraph: A flexible and efficient distributed framework for GNN training,
Lei Wang (Alibaba Group, China), Qiang Yin (Alibaba Group), Chao Tian (Alibaba Group), Jianbang Yang (Shanghai Jiao Tong University), Rong Chen (Shanghai Jiao Tong University, China), Wenyuan Yu (Alibaba Group, China), Zihang Yao (Shanghai Jiao Tong University), Jingren Zhou (Alibaba Group), Qiang Yin (Alibaba Group, China).

DGCL: An Efficient Communication Library for Distributed GNN Training,
Zhenkun Cai (The Chinese University of Hong Kong), Xiao Yan (Southern University of Science and Technology), Yidi Wu (The Chinese University of Hong Kong), Kaihao Ma (The Chinese University of Hong Kong), James Cheng (The Chinese University of Hong Kong), Fan Yu (Huawei Technologies Co. Ltd).

Seastar: Vertex-Centric Programming for Graph Neural Networks,
Yidi Wu (The Chinese University of Hong Kong), Kaihao Ma (The Chinese University of Hong Kong), Zhenkun Cai (The Chinese University of Hong Kong), Tatiana Jin (The Chinese University of Hong Kong), Boyang Li (The Chinese University of Hong Kong, China), Chenguang Zheng (The Chinese University of Hong Kong, China), James Cheng (The Chinese University of Hong Kong), Fan Yu (Huawei Technologies Co. Ltd).

Accelerating Graph Sampling for Graph Machine Learning using GPUs,
Abhinav Jangda (University of Massachusetts Amherst, United States of America), Sandeep Polisetty (University of Massachusetts Amherst), Arjun Guha (Northeastern University, United States of America), Marco Serafini (University of Massachusetts Amherst, United States of America).

Rubberband: Cloud-based Hyperparameter Tuning,
Richard Liaw (UC Berkeley), Ujval Misra (UC Berkeley), Lisa Dunlap (UC Berkeley), Joseph Gonzalez (UC Berkeley, United States of America), Ion Stoica (UC Berkeley, United States of America), Alexey Tumanov (Georgia Tech, United States of America), Kirthevasan Kandasamy (UC Berkeley), Romil Bhardwaj (UC Berkeley, United States of America).


Tahoe: Tree Structure-Aware High Performance Inference Engine for Decision Tree Ensemble on GPU,
Zhen Xie (University of California, Merced), Wenqian Dong (University of California, Merced), Jiawen Liu (University of California, Merced), Hang Liu (Stevens Institute of Technology, United States of America), Dong Li (University of California, Merced).

### 2022

Fleche: An Efficient GPU Embedding Cache for Personalized Recommendations.
Minhui Xie, Youyou Lu, Jiazhen Lin, Qing Wang, and Jian Gao (Tsinghua University), Kai Ren (Kuaishou Technology), Jiwu Shu (Tsinghua University)

GNNLab: A Factored System for Sample-based GNN Training over GPUs.
Jianbang Yang (IPADS, Shanghai Jiao Tong University), Dahai Tang (Hunan University), Xiaoniu Song (IPADS, Shanghai Jiao Tong University, Shanghai AI Laboratory), Lei Wang (Alibaba Group), Qiang Yin (BASICS, Shanghai Jiao Tong University), Rong Chen (IPADS, Shanghai Jiao Tong University, Shanghai AI Laboratory), Wenyuan Yu and Jingren Zhou (Alibaba Group)

Out-Of-Order BackProp: An Effective Scheduling Technique for Deep Learning.
Hyungjun Oh, Junyeol Lee, Hyeongju Kim, and Jiwon Seo (Hanyang University)

### 2023

SiloD: A Co-design of Caching and Scheduling for Deep Learning Clusters    Hanyu Zhao (Peking University), Zhenhua Han (Microsoft Research), Zhi Yang (Peking University), Quanlu Zhang (Microsoft Research), Mingxia Li (USTC), Fan Yang (Microsoft Research), Qianxi Zhang (Microsoft Research), Binyang Li (Microsoft), Yuqing Yang (Microsoft Research), Lili Qiu (Microsoft Research), Lintao Zhang (BaseBit Technologies), Lidong Zhou (Microsoft Research)

MariusGNN: Resource-Efficient Out-of-Core Training of Graph Neural Networks    Roger Waleffe (University of Wisconsin-Madison), Jason Mohoney (University of Wisconsin-Madison), Theodoros Rekatsinas (ETH Zurich), Shivaram Venkataraman (University of Wisconsin-Madison)

Hi-Speed DNN Training with Espresso: Unleashing the Full Potential of Gradient Compression with Near-Optimal Usage Strategies    Zhuang Wang (Rice University), Haibin Lin (ByteDance Inc.), Yibo Zhu (ByteDance Inc.), T. S. Eugene Ng (Rice University)

Fast and Efficient Model Serving Using Multi-GPUs with Direct-Host-Access    Jinwoo Jeong (Ajou University), Seungsu Baek (Ajou University), Jeongseob Ahn (Ajou University)

Tabi: An Efficient Multi-Level Inference System for Large Language Models    Yiding Wang (Hong Kong University of Science and Technology), Kai Chen (Hong Kong University of Science and Technology), Haisheng Tan (University of Science and Technology of China), Kun Guo (Fuzhou University)

JOG: Joint Graph and Operator level Optimizations for Deep Learning Compilation    Zhiying Xu (Nanjing University), Jiafan Xu (Nanjing University), Hongding Peng (Nanjing University), Wei Wang (Nanjing University), Xiaoliang Wang (Nanjing University), Haoran Wan (Nanjing University), Haipeng Dai (Nanjing University), Yixu Xu (Huawei Technologies), Hao Cheng (Huawei Technologies), Kun Wang (The Hong Kong Polytechnic University), Guihai Chen (Nanjing University)

Lyra: Elastic Scheduling for Deep Learning Clusters    Jiamin Li (City University of Hong Kong), Hong Xu (The Chinese University of Hong Kong), Yibo Zhu (ByteDance Inc.), Zherui Liu (ByteDance Inc.), Chuanxiong Guo, Cong Wang (City University of Hong Kong)

Egeria: Efficient DNN Training with Knowledge-Guided Layer Freezing    Yiding Wang (Hong Kong University of Science and Technology), Decang Sun (Hong Kong University of Science and Technology), Kai Chen (Hong Kong University of Science and Technology), Fan Lai (University of Michigan), Mosharaf Chowdhury (University of Michigan)

Pocket: ML Serving from the Edge    Misun Park (Georgia Institute of Technology), Ketan Bhardwaj (Georgia Institute of Technology), Ada Gavrilovska (Georgia Institute of Technology)

## ATC 

### 2018
Locality-Aware Software Throttling for Sparse Matrix Operation on GPUs
Yanhao Chen and Ari B. Hayes, Rutgers University; Chi Zhang, University of Pittsburgh; Timothy Salmon and Eddy Z. Zhang, Rutgers University

Litz: Elastic Framework for High-Performance Distributed Machine Learning
Aurick Qiao, Petuum, Inc. and Carnegie Mellon University; Abutalib Aghayev, Carnegie Mellon University; Weiren Yu, Petuum, Inc. and Beihang University; Haoyang Chen and Qirong Ho, Petuum, Inc.; Garth A. Gibson, Carnegie Mellon University and Vector Institute; Eric P. Xing, Petuum, Inc. and Carnegie Mellon University

Cavs: An Efficient Runtime System for Dynamic Neural Networks
Shizhen Xu, Carnegie Mellon University, Tsinghua University; Hao Zhang, Graham Neubig, and Wei Dai, Carnegie Mellon University, Petuum Inc.; Jin Kyu Kim, Carnegie Mellon University; Zhijie Deng, Tsinghua University; Qirong Ho, Petuum Inc.; Guangwen Yang, Tsinghua University; Eric P. Xing, Petuum Inc.

DeepCPU: Serving RNN-based Deep Learning Models 10x Faster
Minjia Zhang, Samyam Rajbhandari, Wenhan Wang, and Yuxiong He, Microsoft AI and Research

### 2019

SIMD-X: Programming and Processing of Graph Algorithms on GPUs
Hang Liu, University of Massachusetts Lowell; H. Howie Huang, George Washington University

NeuGraph: Parallel Deep Neural Network Computation on Large Graphs
Lingxiao Ma and Zhi Yang, Peking University; Youshan Miao, Jilong Xue, Ming Wu, and Lidong Zhou, Microsoft Research; Yafei Dai, Peking University

Cognitive SSD: A Deep Learning Engine for In-Storage Data Retrieval
Shengwen Liang and Ying Wang, State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing; University of Chinese Academy of Sciences; Youyou Lu and Zhe Yang, Tsinghua University; Huawei Li and Xiaowei Li, State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing; University of Chinese Academy of Sciences

Analysis of Large-Scale Multi-Tenant GPU Clusters for DNN Training Workloads
Myeongjae Jeon, UNIST and Microsoft Research; Shivaram Venkataraman, University of Wisconsin and Microsoft Research; Amar Phanishayee and Junjie Qian, Microsoft Research; Wencong Xiao, Beihang University and Microsoft Research; Fan Yang, Microsoft Research

Optimizing CNN Model Inference on CPUs
Yizhi Liu, Yao Wang, Ruofei Yu, Mu Li, Vin Sharma, and Yida Wang, Amazon


MArk: Exploiting Cloud Services for Cost-Effective, SLO-Aware Machine Learning Inference Serving
Chengliang Zhang, Minchen Yu, and Wei Wang, Hong Kong University of Science and Technology; Feng Yan, University of Nevada, Reno

### 2020

HetPipe: Enabling Large DNN Training on (Whimpy) Heterogeneous GPU Clusters through Integration of Pipelined Model Parallelism and Data Parallelism
Jay H. Park, Gyeongchan Yun, Chang M. Yi, Nguyen T. Nguyen, and Seungmin Lee, UNIST; Jaesik Choi, KAIST; Sam H. Noh and Young-ri Choi, UNIST

AutoSys: The Design and Operation of Learning-Augmented Systems
Chieh-Jan Mike Liang, Hui Xue, Mao Yang, and Lidong Zhou, Microsoft Research; Lifei Zhu, Peking University and Microsoft Research; Zhao Lucis Li and Zibo Wang, University of Science and Technology of China and Microsoft Research; Qi Chen and Quanlu Zhang, Microsoft Research; Chuanjie Liu, Microsoft Bing Platform; Wenjun Dai, Microsoft Bing Ads

Daydream: Accurately Estimating the Efficacy of Optimizations for DNN Training
Hongyu Zhu, University of Toronto & Vector Institute; Amar Phanishayee, Microsoft Research; Gennady Pekhimenko, University of Toronto & Vector Institute

NeuOS: A Latency-Predictable Multi-Dimensional Optimization Framework for DNN-driven Autonomous Systems
Soroush Bateni and Cong Liu, University of Texas at Dallas

Scaph: Scalable GPU-Accelerated Graph Processing with Value-Driven Differential Scheduling
Long Zheng, Xianliang Li, Yaohui Zheng, Yu Huang, Xiaofei Liao, and Hai Jin, Huazhong University of Science and Technology; Jingling Xue, UNSW Sydney; Zhiyuan Shao and Qiang-Sheng Hua, Huazhong University of Science and Technology

### 2021


Octo: INT8 Training with Loss-aware Compensation and Backward Quantization for Tiny On-device Learning
Qihua Zhou and Song Guo, Hong Kong Polytechnic University; Zhihao Qu, Hohai University; Jingcai Guo, Zhenda Xu, Jiewei Zhang, Tao Guo, and Boyuan Luo, Hong Kong Polytechnic University; Jingren Zhou, Alibaba Group

Fine-tuning giant neural networks on commodity hardware with automatic pipeline model parallelism
Saar Eliad, Ido Hakimi, and Alon De Jagger, Department of Computer Science, Technion - Israel Institute of Technology; Mark Silberstein, Department of Computer Science and Department of Electrical Engineering, Technion - Israel Institute of Technology; Assaf Schuster, Department of Computer Science, Technion - Israel Institute of Technology

INFaaS: Automated Model-less Inference Serving
Francisco Romero, Qian Li, Neeraja J. Yadwadkar, and Christos Kozyrakis, Stanford University

Jump-Starting Multivariate Time Series Anomaly Detection for Online Service Systems
Minghua Ma, Tsinghua University, BNRist; Shenglin Zhang, Nankai University; Junjie Chen, Tianjin University; Jim Xu, Georgia Tech; Haozhe Li and Yongliang Lin, Nankai University; Xiaohui Nie, Tsinghua University, BNRist; Bo Zhou and Yong Wang, CNCERT/CC; Dan Pei, Tsinghua University, BNRist

Palleon: A Runtime System for Efficient Video Processing toward Dynamic Class Skew
Boyuan Feng, Yuke Wang, Gushu Li, Yuan Xie, and Yufei Ding, University of California, Santa Barbara

Habitat: A Runtime-Based Computational Performance Predictor for Deep Neural Network Training
Geoffrey X. Yu, University of Toronto/Vector Institute; Yubo Gao, University of Toronto; Pavel Golikov and Gennady Pekhimenko, University of Toronto/Vector Institute


Zico: Efficient GPU Memory Sharing for Concurrent DNN Training
Gangmuk Lim, UNIST; Jeongseob Ahn, Ajou University; Wencong Xiao, Alibaba Group; Youngjin Kwon, KAIST; Myeongjae Jeon, UNIST

Refurbish Your Training Data: Reusing Partially Augmented Samples for Faster Deep Neural Network Training
Gyewon Lee, Seoul National University and FriendliAI; Irene Lee, Georgia Institute of Technology; Hyeonmin Ha, Kyunggeun Lee, and Hwarim Hyun, Seoul National University; Ahnjae Shin and Byung-Gon Chun, Seoul National University and FriendliAI

ZeRO-Offload: Democratizing Billion-Scale Model Training
Jie Ren, UC Merced; Samyam Rajbhandari, Reza Yazdani Aminabadi, and Olatunji Ruwase, Microsoft; Shuangyan Yang, UC Merced; Minjia Zhang, Microsoft; Dong Li, UC Merced; Yuxiong He, Microsoft

### 2022

Faith: An Efficient Framework for Transformer Verification on GPUs
Boyuan Feng, Tianqi Tang, Yuke Wang, Zhaodong Chen, Zheng Wang, Shu Yang, Yuan Xie, Yufei Ding, University of California, Santa Barbara

DVABatch: Diversity-aware Multi-Entry Multi-Exit Batching for Efficient Processing of DNN Services on GPUs
Weihao Cui, Han Zhao, Quan Chen, Hao Wei, and Zirui Li, Shanghai Jiao Tong University; Deze Zeng, China University of Geosciences; Chao Li and Minyi Guo, Shanghai Jiao Tong University

Serving Heterogeneous Machine Learning Models on Multi-GPU Servers with Spatio-Temporal Sharing
Seungbeom Choi, Sunho Lee, Yeonjae Kim, Jongse Park, Youngjin Kwon, and Jaehyuk Huh, KAIST

PilotFish: Harvesting Free Cycles of Cloud Gaming with Deep Learning Training
Wei Zhang and Binghao Chen, Shanghai Jiao Tong University; Zhenhua Han, Microsoft Research; Quan Chen, Shanghai Jiao Tong University; Peng Cheng, Fan Yang, Ran Shu, and Yuqing Yang, Microsoft Research; Minyi Guo, Shanghai Jiao Tong University

Tetris: Memory-efficient Serverless Inference through Tensor Sharing
Jie Li, Laiping Zhao, and Yanan Yang, Tianjin University; Kunlin Zhan, 58.com; Keqiu Li, Tianjin University

PetS: A Unified Framework for Parameter-Efficient Transformers Serving
Zhe Zhou, Peking University; Xuechao Wei, Peking University, Alibaba Group; Jiejing Zhang, Alibaba Group; Guangyu Sun, Peking University


Campo: Cost-Aware Performance Optimization for Mixed-Precision Neural Network Training
Xin He, CSEE, Hunan University & Xidian University; Jianhua Sun and Hao Chen, CSEE, Hunan University; Dong Li, University of California, Merced

Primo: Practical Learning-Augmented Systems with Interpretable Models
Qinghao Hu, Nanyang Technological University; Harsha Nori, Microsoft; Peng Sun, SenseTime; Yonggang Wen and Tianwei Zhang, Nanyang Technological University

Cachew: Machine Learning Input Data Processing as a Service
Dan Graur, Damien Aymon, Dan Kluser, and Tanguy Albrici, ETH Zurich; Chandramohan A. Thekkath, Google; Ana Klimovic, ETH Zurich


SOTER: Guarding Black-box Inference for General Neural Networks at the Edge
Tianxiang Shen, Ji Qi, Jianyu Jiang, Xian Wang, Siyuan Wen, Xusheng Chen, and Shixiong Zhao, The University of Hong Kong; Sen Wang and Li Chen, Huawei Technologies; Xiapu Luo, The Hong Kong Polytechnic University; Fengwei Zhang, Southern University of Science and Technology (SUSTech); Heming Cui, The University of Hong Kong


## VLDB

### 2021
tf.data: a machine learning data processing framework
Derek G. Murray, Jiří Šimša, Ana Klimovic, Ihor Indyk

Analyzing and Mitigating Data Stalls in DNN Training
Jayashree Mohan, Amar Phanishayee, Ashish Raniwala, Vijay Chidambaram

### 2022
Hippo: sharing computations in hyper-parameter optimization.
Ahnjae Shin, Joo Seong Jeong, Do Yoon Kim, Soyoung Jung, Byung-Gon Chun

WindTunnel: Towards Differentiable ML Pipelines Beyond a Single Model.
Gyeong-In Yu, Saeed Amizadeh, Sehoon Kim, Artidoro Pagnoni, Ce Zhang, Byung-Gon Chun, Markus Weimer, Matteo Interlandi 

## SIGMOD

### 2022

HET-GMP: a Graph-based System Approach to Scaling Large Embedding Model Training
Xupeng Miao (Peking University)*; Yining Shi (Peking University); Hailin Zhang (Peking University); Xin Zhang (Peking University); Xiaonan Nie (Peking University); Zhi Yang (Peking University); Bin Cui (Peking University)

Where Is My Training Bottleneck? Hidden Trade-Offs in Deep Learning Preprocessing Pipelines
Alexander Isenko (Technical University of Munich)*; Ruben Mayer (Technical University of Munich); Jeffery Jedele (Technical University of Munich); Hans-Arno Jacobsen (University of Toronto)

Complaint-Driven Training Data Debugging at Interactive Speeds
Lampros Flokas (Columbia University)*; Weiyuan Wu (Simon Fraser University); Yejia Liu (Simon Fraser University); Jiannan Wang (Simon Fraser University); Nakul Verma (Columbia University); Eugene Wu (Columbia University)

FuseME: Distributed Matrix Computation Engine based on Cuboid-based Fused Operator and Plan Generation
Donghyoung Han (KAIST)*; Jongwuk Lee (Sungkyunkwan University); Min-Soo Kim (KAIST)

Sommelier: Curating DNN Models for the Masses
Peizhen Guo (Yale University)*; Bo Hu (Yale University); Wenjun Hu (Yale University)

BlindFL: Vertical Federated Machine Learning without Peeking into Your Data
"Fangcheng Fu (Peking University)*; Huanran Xue (Tencent Inc.); Yong Cheng ( Tencent Inc.); Yangyu Tao (Tencent Inc.); Bin Cui (Peking University)"

NeutronStar: Distributed GNN Training with Hybrid Dependency Management
Qiange Wang (Northeastern University); Yanfeng Zhang (NorthEastern University)*; Hao Wang (the Ohio State University); Chaoyi Chen (Northeastern University); Xiaodong Zhang (Ohio State U.); Ge Yu (Northeast University)

End-to-end Optimization of Machine Learning Prediction Queries
Kwanghyun Park (Microsoft)*; Karla Saur (Microsoft); Dalitso Banda (Microsoft); Rathijit Sen (Microsoft); Matteo Interlandi (Microsoft); Konstantinos Karanasos (Microsoft)


## NeurIPS



### 2015

Hidden Technical Debt in Machine Learning Systems.

D. Sculley, Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Dietmar Ebner, Vinay Chaudhary, Michael Young, Jean-Franc¸ois Crespo, Dan Dennison.

### 2018

Mesh-TensorFlow: Deep Learning for Supercomputers Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, Ryan Sepassi, Blake Hechtman

### 2019
PyTorch: An Imperative Style, High-Performance Deep Learning Library Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, Soumith Chintala

GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, zhifeng Chen


### 2020
Nimble: Lightweight and Parallel GPU Task Scheduling for Deep Learning Woosuk Kwon, Gyeong-In Yu, Eunji Jeong, Byung-Gon Chun

PyGlove: Symbolic Programming for Automated Machine Learning Daiyi Peng, Xuanyi Dong, Esteban Real, Mingxing Tan, Yifeng Lu, Gabriel Bender, Hanxiao Liu, Adam Kraft, Chen Liang, Quoc Le

### 2021

Terra: Imperative-Symbolic Co-Execution of Imperative Deep Learning Programs Taebum Kim, Eunji Jeong, Geon-Woo Kim, Yunmo Koo, Sehoon Kim, Gyeongin Yu, Byung-Gon Chun

## ICML

### 2021
TeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale Language Models
Zhuohan Li · Siyuan Zhuang · Shiyuan Guo · Danyang Zhuo · Hao Zhang · Dawn Song · Ion Stoica



## ICLR

### 2021

GShard: Scaling giant models with conditional computation and automatic sharding.

[Dmitry Lepikhin](https://arxiv.org/search/cs?searchtype=author&query=Lepikhin%2C+D), [HyoukJoong Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+H), [Yuanzhong Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Y), [Dehao Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+D), [Orhan Firat](https://arxiv.org/search/cs?searchtype=author&query=Firat%2C+O), [Yanping Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang%2C+Y), [Maxim Krikun](https://arxiv.org/search/cs?searchtype=author&query=Krikun%2C+M), [Noam Shazeer](https://arxiv.org/search/cs?searchtype=author&query=Shazeer%2C+N), Zhifeng Chen



## ASPLOS

### 2019

Swizzle Inventor: Data Movement Synthesis for GPU Kernels 
Phitchaya Mangpo Phothilimthana (University of California, Berkeley);Archibald Samuel Elliott (University of Washington);Abhinav Jangda (University of Massachusetts Amherst);Bastian Hagedorn (University of Münster);Henrik Barthels (AICES, RWTH Aachen University);Rastislav Bodik (University of Washington);Vinod Grover (NVIDIA)

DeepSigns: An End-to-End Watermarking Framework for Protecting the Ownership of Deep Neural Networks
Bita Darvish Rouhani, Huili Chen, Farinaz Koushanfar(UC San Diego)

DiGraph: An Efficient Path-based Iterative Directed Graph Processing System on Multiple GPUs
Yu Zhang, Xiaofei Liao, Hai Jin (Huazhong University of Science and Technology);Bingsheng He (National University of Singapore);Haikun Liu, Lin Gu (Huazhong University of Science and Technology)

TANGRAM: Optimized Coarse-Grained Dataflow for Scalable NN Accelerators
Mingyu Gao, Xuan Yang, Jing Pu, Mark Horowitz, Christos Kozyrakis (Stanford University)

Packing Sparse Convolutional Neural Networks for Efficient Systolic Array Implementations: Column Combining Under Joint Optimization
HT Kung, Bradley McDanel, Sai Qian Zhang (Harvard University)

Split-CNN: Splitting Window-based Operations in Convolutional Neural Networks for Memory System Optimization 
Tian Jin (IBM T.J. Watson Research Center);Seokin Hong (Kyungpook National University)

HOP: Heterogeneity-Aware Decentralized Training
Qinyi Luo (University of Southern California);Jinkun Lin (Tsinghua University);Youwei Zhuo, Xuehai Qian (University of Southern California)

Astra: Exploiting Predictability to Optimize Deep Learning\
Muthian Sivathanu (Microsoft Research India);Tapan Chugh (Microsoft Research India);Sanjay Srivallabh (Microsoft Research India);Lidong Zhou (Microsoft Research Asia)

ADMM-NN: An Algorithm-Hardware Co-Design Framework of DNNs Using Alternating Direction Methods of Multipliers 
Ao Ren (Northeastern University);Jiayu Li, Tianyun Zhang, Shaokai Ye (Syracuse University);Wenyao Xu (SUNY, Buffalo);Xuehai Qian (University of Southern California);Xue Lin, Yanzhi Wang (Northeastern University)

### 2020

Interstellar: Using Halide’s Scheduling Language to Analyze DNN Accelerators
Xuan Yang (Stanford University); Mingyu Gao (Tsinghua University); Qiaoyi Liu (Stanford University); Jeff Setter (Stanford University); Jing Pu (Stanford University); Ankita Nayak (Stanford University); Steven Bell (Stanford University); Kaidi Cao (Stanford University); Heonjae Ha (Stanford University); Priyanka Raina (Stanford University); Christos Kozyrakis (Stanford University, Google); Mark Horowitz (Stanford University)

DeepSniffer: A DNN Model Extraction Framework Based on Learning
Architectural Hints
Xing Hu (University of California, Santa Barbara); Ling Liang (University of California, Santa Barbara); Shuangchen Li (University of California, Santa Barbara); Lei Deng (University of California, Santa Barbara & Tsinghua University); Pengfei Zuo (University of California, Santa Barbara & Huazhong University of Science and Technology); Yu Ju (University of California, Santa Barbara & Tsinghua University); Xinfeng Xie (University of California, Santa Barbara); Yufei Ding (University of California, Santa Barbara); Chang Liu (Citadel Securities); Timothy Sherwood (University of California, Santa Barbara); Yuan Xie (University of California, Santa Barbara)

Prague: High-Performance Heterogeneity-Aware Asynchronous Decentralized Training    
Qinyi Luo (University of Southern California); Jiaao He (Tsinghua University); Youwei Zhuo (University of Southern California); Xuehai Qian (University of Southern California)

FlexTensor: An Automatic Schedule Exploration and Optimization Framework for Tensor Computation on Heterogeneous System
Size Zheng (Peking University); Yun Liang (Peking University); Shuo Wang (Peking University); Renze Chen (Peking University); Kaiwen Sheng (Peking University)



AutoTM: Automatic Tensor Movement in Heterogeneous Memory Systems using Integer Linear Programming
Mark Hildebrand (University of California, Davis); Jawad Khan (Intel Corporation); Sanjeev Trika (Intel Corporation); Jason Lowe-Power (University of California, Davis); Venkatesh Akella (University of California, Davis)

Capuchin: Tensor-based GPU Memory Management for Deep Learning
Xuan Peng (Huazhong University of Science and Technology); Xuanhua Shi (Huazhong University of Science and Technology); Hulin Dai (Huazhong University of Science and Technology); Hai Jin (Huazhong University of Science and Technology); Weiliang Ma (Huazhong University of Science and Technology); Qian Xiong (Huazhong University of Science and Technology); Fan Yang (Microsoft Research Asia); Xuehai Qian (University of Southern California)

SwapAdvisor: Pushing Deep Learning Beyond the GPU Memory Limit via Smart Swapping
Chien-Chin Huang (New York University); Gu Jin (New York University); Jinyang Li (New York University)

### 2021

Analytical Characterization and Design Space Exploration for Optimization of CNNs
Rui Li, Yufan Xu (University of Utah); Aravind Sukumaran-Rajam (Washington State University); Atanas Rountev (Ohio State University); P. Sadayappan (University of Utah)

### 2022

A Full-stack Search Technique for Domain Optimized Deep Learning Accelerators
Dan Zhang (Google Brain), Safeen Huda (Google), Ebrahim Songhori (Google Brain), Kartik Prabhu (Stanford University), Quoc Le (Google Brain), Anna Goldie (Google Brain), Azalia Mirhoseini (Google Brain)

ValueExpert: Exploring Value Patterns in GPU-accelerated Applications
Keren Zhou (Rice University), Yueming Hao (North Carolina State University), John Mellor-Crummey (Rice University), Xiaozhu Meng (Rice University), Xu Liu (North Carolina State University), (Oak Ridge National Laboratory)

RecShard: Statistical Feature-Based Memory Optimization for Industry-Scale Neural Recommendation
Geet Sethi (Stanford University), (Meta), Bilge Acun (Meta), Niket Agarwal (Meta), Christos Kozyrakis (Stanford University), Caroline Trippel (Stanford University), Carole-Jean Wu (Meta)

AStitch: Enabling A New Multi-Dimensional Optimization Space for Memory-Intensive ML Training and Inference on Modern SIMT Architectures
Zhen Zheng (Alibaba Group), Xuanda Yang (Alibaba Group), Pengzhan Zhao (Alibaba Group), Guoping Long (Alibaba Group), Kai Zhu (Alibaba Group), Feiwen Zhu (Alibaba Group), Wenyi Zhao (Alibaba Group), Xiaoyong Liu (Alibaba Group), Jun Yang (Alibaba), Jidong Zhai (Tsinghua University), Shuaiwen Leon Song (University of Sydney & University of Washington), Wei Lin (Alibaba Group)

NASPipe: High Performance and Reproducible Pipeline Parallel Supernet Training via Causal Synchronous Parallelism
Shixiong Zhao (University of Hong Kong), Fanxin Li (University of Hong Kong), Xusheng Chen (University of Hong Kong), Tianxiang Shen (University of Hong Kong), Li Chen (Huawei Technologies), Sen Wang (Huawei Technologies), Nicholas Zhang (Huawei Technologies), Cheng Li (University of Science and Technology of China), Heming Cui (University of Hong Kong)

VELTAIR: Towards High-Performance Multi-Tenant Deep Learning Services via Adaptive Compilation and Scheduling
Zihan Liu (Shanghai Jiao Tong University), Jingwen Leng (Shanghai Jiao Tong University), Zhihui Zhang (Shanghai Jiao Tong University), Quan Chen (Shanghai Jiao Tong University), Chao Li (Shanghai Jiao Tong University), Minyi Guo (Shanghai Jiao Tong University)

Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads
Abhinav Jangda (University of Massachusetts at Amherst), Jun Huang (Ohio State University), Guodong Liu (Chinese Academy of Sciences), Amir Hossein Nodehi Sabet (University of California at Riverside), Saeed Maleki (Microsoft Research), Youshan Miao (Microsoft Research), Madanlal Musuvathi (Microsoft Research), Todd Mytkowicz (Microsoft Research), Olli Saarikivi (Microsoft Research)

Astraea: Towards QoS-Aware and Resource-Efficient Multi-stage GPU Services
Wei Zhang (Shanghai Jiao Tong University), Quan Chen (Shanghai Jiao Tong University), Kaihua Fu (Shanghai Jiao Tong University), Ningxin Zheng (Microsoft Research), Zhiyi Huang (University of Otago), Jingwen Leng (Shanghai Jiao Tong University), Minyi Guo (Shanghai Jiao Tong University)

SOL: Safe On-Node Learning in Cloud Platforms
Yawen Wang (Stanford University), Daniel Crankshaw (Microsoft Research), Neeraja J. Yadwadkar (University of Texas at Austin), Daniel Berger (Microsoft Research), Christos Kozyrakis (Stanford University), Ricardo Bianchini (Microsoft Research)


## SC

### 2021

KAISA: An Adaptive Second-Order Optimizer Framework for Deep Neural Networks
J. Gregory Pauloski,Qi Huang,Lei Huang,Shivaram Venkataraman,Kyle ChardIan Foster,Zhao Zhang

Tensor Processing Primitives: A Programming Abstraction for Efficiency and Portability in Deep Learning Workloads
Evangelos Georganas,Dhiraj Kalamkar,Sasikanth Avancha,Menachem Adelman,Cristina Anderson,Alexander Breuer,Jeremy Bruestle,Narendra Chaudhary,Abhisek Kundu,Denise Kutnick,Frank Laub,Vasimuddin MdSanchit Misra,Ramanarayan Mohanty,Hans Pabst,Barukh Ziv,Alexander Heinecke

Enable Simultaneous DNN Services Based on Deterministic Operator Overlap and Precise Latency Prediction
Weihao Cui,Han Zhao,Quan Chen,Ningxin Zheng,Jingwen Leng,Jieru Zhao,Zhuo Song,Tao Ma,Yong Yang,Chao Li,Minyi Guo

ET: Re-Thinking Self-Attention for Transformer Models on GPUs
Shiyang Chen, Shaoyi Huang, Santosh Pandey, Bingbing Li, Guang R. Gao, Long Zheng, Caiwen Ding, Hang Liu

Parallel Construction of Module Networks
Ankit Srivastava, Sriram Chockalingam, Maneesha Aluru, Srinivas Aluru

Chimera: Efficiently Training Large-Scale Neural Networks with Bidirectional Pipelines
Shigang Li, Torsten Hoefler

APNN-TC: Accelerating Arbitrary Precision Neural Networks on Ampere GPU Tensor Cores
Boyuan Feng, Yuke Wang, Tong Geng, Ang Li, Yufei Ding

Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM
Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, Amar Phanishayee, Matei Zaharia

ZeRO-infinity: breaking the GPU memory wall for extreme scale deep learning
Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, Yuxiong He

FedAT: A High-Performance and Communication-Efficient Federated Learning System with Asynchronous Tiers
Zheng Chai, Yujing Chen, Ali Anwar,Liang Zhao,Yue Cheng,Huzefa Rangwala

DistGNN: Scalable Distributed Training for Large-Scale Graph Neural Networks
Vasimuddin Md,Sanchit Misra,Guixiang MaRamanarayan Mohanty,Evangelos Georganas,Alexander Heinecke,Dhiraj Kalamkar,Nesreen K. Ahmed,Sasikanth Avancha

Efficient Scaling of Dynamic Graph Neural Networks
Venkatesan T. Chakaravarthy,Shivmaran S. Pandian,Saurabh Raje,Yogish Sabharwal,Toyotaro Suzumura,Shashanka Ubaru
Efficient Tensor Core-Based GPU Kernels for Structured Sparsity Under Reduced Precision
Zhaodong Chen,Zheng Qu,Liu Liu,Yufei Ding,Yuan Xie

MAPA: Multi-Accelerator Pattern Allocation Policy for Multi-Tenant GPU Servers
Kiran Ranganath, Joshua D. Suetterlein,Joseph Manzano,Shuaiwen Leon Song, Daniel Wong

Online Evolutionary Batch Size Orchestration for Scheduling Deep Learning Workloads in GPU Clusters
Zhengda Bian, Shenggui Li, Wei Wang, Yang You


### 2022

Efficient Quantized Sparse Matrix Operations on Tensor Cores
Shigang Li, Kazuki Osawa, Torsten Hoefler

LightSeq2: Accelerated Training for Transformer-Based Models on GPUs
Xiaohui Wang, Yang Wei, Ying Xiong, Guyue Huang, Xian Qian, Yufei Ding, Mingxuan Wang, Lei Li

CoGNN: Efficient Scheduling for Concurrent GNN Training on GPUs
Qingxiao Sun,Yi Liu,Hailong Yang,Ruizhe Zhang,Ming Dun,Mingzhen Li,Xiaoyan Liu,Wencong Xiao,Yong Li,Zhongzhi Luan,Depei Qian

DeepSpeed-Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale
Reza Yazdani Aminabadi,Samyam Rajbhandari,Ammar Ahmad Awan,Cheng Li,Du Li,Elton Zheng,Olatunji Ruwase,Shaden Smith,Minjia Zhang,Jeff Rasley,Yuxiong He

VSGM: View-Based GPU-Accelerated Subgraph Matching on Large Graphs
Guanxian Jiang,Qihui Zhou,Tatiana Jin,Boyang Li,Yunjian Zhao,Yichao Li,James Cheng

STMatch: Accelerating Graph Pattern Matching on GPU with Stack-Based Loop Optimizations
Yihua Wei, Peng Jiang

WholeGraph: A Fast Graph Neural Network Training Framework with Multi-GPU Distributed Shared Memory Architecture
Dongxu Yang,Junhong Liu,Jiaxing Qi,Junjie Lai

SpDISTAL: Compiling Distributed Sparse Tensor Computations
Rohan Yadav,Alex Aiken,Fredrik Kjolstad

EL-Rec: Efficient Large-Scale Recommendation Model Training via Tensor-Train Embedding Table
Zheng Wang,Yuke Wang,Boyuan Feng,Dheevatsa Mudigere,Bharath Muthiah,Yufei Ding

STRONGHOLD: Fast and Affordable Billion-Scale Deep Learning Model Training
Xiaoyang Sun,Wei Wang,Shenghao Qiu,Renyu Yang,Songfang Huang,Jie Xu,Zheng Wang

HGL: Accelerating Heterogeneous GNN Training with Holistic Representation and Optimization
Yuntao Gui,Yidi Wu,Han Yang,Tatiana Jin,Boyang Li,Qihui Zhou,James Cheng,Fan Yu



## HPCA

### 2018

Applied Machine Learning at Facebook: A Datacenter Infrastructure Perspective.
[Kim M. Hazelwood](https://dblp.org/pid/77/1117.html), [Sarah Bird](https://dblp.org/pid/53/8277.html), [David M. Brooks](https://dblp.org/pid/30/135.html), [Soumith Chintala](https://dblp.org/pid/57/8662.html), [Utku Diril](https://dblp.org/pid/217/0471.html), [Dmytro Dzhulgakov](https://dblp.org/pid/217/0550.html), [Mohamed Fawzy](https://dblp.org/pid/217/0652.html), [Bill Jia](https://dblp.org/pid/177/3681.html), [Yangqing Jia](https://dblp.org/pid/19/2618.html), [Aditya Kalro](https://dblp.org/pid/199/2110.html), [James Law](https://dblp.org/pid/74/4292.html), [Kevin Lee](https://dblp.org/pid/44/765.html), [Jason Lu](https://dblp.org/pid/63/1593.html), [Pieter Noordhuis](https://dblp.org/pid/45/10283.html), [Misha Smelyanskiy](https://dblp.org/pid/22/4090.html), [Liang Xiong](https://dblp.org/pid/48/305.html), Xiaodong Wang



### 2020

The Architectural Implications of Facebook’s DNN-based Personalized Recommendation.

Udit Gupta, Carole-Jean Wu, Xiaodong Wang, Maxim Naumov, Brandon Reagen David Brooks, Bradford Cottel, Kim Hazelwood, Mark Hempstead, Bill Jia, Hsien-Hsin S. Lee, Andrey Malevich, Dheevatsa Mudigere, Mikhail Smelyanskiy, Liang Xiong, Xuan Zhang




## MLSys



### 2018

Compiling machine learning programs via high-level tracing.

Roy Frostig, Matthew James Johnson, Chris Leary.



### 2019

Beyond Data and Model Parallelism for Deep Neural Networks.
Zhihao Jia, Matei Zaharia, Alex Aiken

### 2020

MLPerf Training Benchmark.

*Peter Mattson, Christine Cheng, Gregory Diamos, Cody Coleman, Paulius Micikevicius, David Patterson, Hanlin Tang, Gu-Yeon Wei, Peter Bailis, Victor Bittorf, David Brooks, Dehao Chen, Debo Dutta, Udit Gupta, Kim Hazelwood, Andy Hock, Xinyuan Huang, Daniel Kang, David Kanter, Naveen Kumar, Jeffery Liao, Deepak Narayanan, Tayo Oguntebi, Gennady Pekhimenko, Lillian Pentecost, Vijay Janapa Reddi, Taylor Robie, Tom St John, Carole-Jean Wu, Lingjie Xu, Cliff Young, Matei Zaharia*



### 2021

Exploring the limits of Concurrency in ML Training on Google TPUs.

[Sameer Kumar](https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+S), [James Bradbury](https://arxiv.org/search/cs?searchtype=author&query=Bradbury%2C+J), [Cliff Young](https://arxiv.org/search/cs?searchtype=author&query=Young%2C+C), [Yu Emma Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+Y+E), [Anselm Levskaya](https://arxiv.org/search/cs?searchtype=author&query=Levskaya%2C+A), [Blake Hechtman](https://arxiv.org/search/cs?searchtype=author&query=Hechtman%2C+B), [Dehao Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+D), [HyoukJoong Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+H), [Mehmet Deveci](https://arxiv.org/search/cs?searchtype=author&query=Deveci%2C+M), [Naveen Kumar](https://arxiv.org/search/cs?searchtype=author&query=Kumar%2C+N), [Pankaj Kanwar](https://arxiv.org/search/cs?searchtype=author&query=Kanwar%2C+P), [Shibo Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+S), [Skye Wanderman-Milne](https://arxiv.org/search/cs?searchtype=author&query=Wanderman-Milne%2C+S), [Steve Lacy](https://arxiv.org/search/cs?searchtype=author&query=Lacy%2C+S), [Tao Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+T), [Tayo Oguntebi](https://arxiv.org/search/cs?searchtype=author&query=Oguntebi%2C+T), [Yazhou Zu](https://arxiv.org/search/cs?searchtype=author&query=Zu%2C+Y), [Yuanzhong Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+Y), Andy Swing



### 2022

Pathways: Asynchronous Distributed Dataflow for ML
Paul Barham, Aakanksha Chowdhery, Jeff Dean, Sanjay Ghemawat, Steven Hand, Daniel Hurt, Michael Isard, Hyeontaek Lim, Ruoming Pang, Sudip Roy, Brennan Saeta, Parker Schuh, Ryan Sepassi, Laurent Shafey , Chandu Thekkath, Yonghui Wu

DietCode: Automatic Optimization for Dynamic Tensor Programs
Bojian Zheng, Ziheng Jiang, Cody Hao Yu, Haichen Shen, Joshua Fromm, Yizhi Liu, Yida Wang, Luis Ceze, Tianqi Chen, Gennady Pekhimenko



## PPoPP

### 2021

Synthesizing Optimal Collective Algorithms
[Zixian Cai](https://ppopp21.sigplan.org/profile/zixiancai) Australian National University, [Zhengyang Liu](https://ppopp21.sigplan.org/profile/zhengyangliu) University of Utah, [Saeed Maleki](https://ppopp21.sigplan.org/profile/saeedmaleki) Microsoft Research, [Madan Musuvathi](https://ppopp21.sigplan.org/profile/madanmusuvathi) Microsoft Research, [Todd Mytkowicz](https://ppopp21.sigplan.org/profile/toddmytkowicz) Microsoft Research, [Jacob Nelson](https://ppopp21.sigplan.org/profile/jacobnelson) Microsoft Research, [Olli Saarikivi](https://ppopp21.sigplan.org/profile/ollisaarikivi1) Microsoft Research, Redmond



**[Understanding and Bridging the Gaps in Current GNN Performance Optimizations](https://ppopp21.sigplan.org/program/program-PPoPP-2021/#)**

[Kezhao Huang](https://ppopp21.sigplan.org/profile/kezhaohuang) Tsinghua University, [Jidong Zhai](https://ppopp21.sigplan.org/profile/jidongzhai) Tsinghua University, [Zhen Zheng](https://ppopp21.sigplan.org/profile/zhenzheng1) Alibaba Group, [Youngmin Yi](https://ppopp21.sigplan.org/profile/youngminyi) University of Seoul, [Xipeng Shen](https://ppopp21.sigplan.org/profile/xipengshen) North Carolina State University



**[I/O Lower Bounds for Auto-tuning of Convolutions in CNNs](https://ppopp21.sigplan.org/program/program-PPoPP-2021/#)**

[Xiaoyang Zhang](https://ppopp21.sigplan.org/profile/xiaoyangzhang) Institute of Computing Technology, Chinese Academy of Sciences, [Junmin Xiao](https://ppopp21.sigplan.org/profile/junminxiao) Institute of Computing Technology, Chinese Academy of Sciences, [Guangming Tan](https://ppopp21.sigplan.org/profile/guangmingtan) Institute of Computing Technology, Chinese Academy of Sciences



**[TurboTransformers: An Efficient GPU Serving System For Transformer Models](https://ppopp21.sigplan.org/program/program-PPoPP-2021/#)**

[Jiarui Fang](https://ppopp21.sigplan.org/profile/jiaruifang) Tencent, [Yang Yu](https://ppopp21.sigplan.org/profile/yangyu) , [Chengduo Zhao](https://ppopp21.sigplan.org/profile/chengduozhao) Tencent, [Jie Zhou](https://ppopp21.sigplan.org/profile/jiezhou1) Tencent



**[DAPPLE: A Pipelined Data Parallel Approach for Training Large Models](https://ppopp21.sigplan.org/program/program-PPoPP-2021/#)**

[Shiqing Fan](https://ppopp21.sigplan.org/profile/shiqingfan1) Alibaba Group, [Yi Rong](https://ppopp21.sigplan.org/profile/yirong) Alibaba Group, [Chen Meng](https://ppopp21.sigplan.org/profile/chenmeng) Alibaba Group, [ZongYan Cao](https://ppopp21.sigplan.org/profile/zongyancao) Alibaba Group, [Siyu Wang](https://ppopp21.sigplan.org/profile/siyuwang) Alibaba Group, [Zhen Zheng](https://ppopp21.sigplan.org/profile/zhenzheng1) Alibaba Group, [Chuan Wu](https://ppopp21.sigplan.org/profile/chuanwu) The University of Hong Kong, [Guoping Long](https://ppopp21.sigplan.org/profile/guopinglong) Alibaba Group, [Jun Yang](https://ppopp21.sigplan.org/profile/junyang) Alibaba Group, [LiXue Xia](https://ppopp21.sigplan.org/profile/lixuexia) Alibaba Group, [Lansong Diao](https://ppopp21.sigplan.org/profile/lansongdiao) Alibaba Group, [Xiaoyong Liu](https://ppopp21.sigplan.org/profile/xiaoyongliu) Alibaba Group, [Wei Lin](https://ppopp21.sigplan.org/profile/weilin1) Alibaba Group



### 2022

**[CASE: A Compiler-Assisted SchEduling Framework for Multi-GPU Systems](https://ppopp22.sigplan.org/program/program-PPoPP-2022/#)**

[Chao Chen](https://ppopp22.sigplan.org/profile/chaochen1) Amazon Web Service, [Chris Porter](https://ppopp22.sigplan.org/profile/chrisporter) Georgia Institute of Technology, USA, [Santosh Pande](https://ppopp22.sigplan.org/profile/santoshpande1) Georgia Institute of Technology



**[TileSpGEMM: A Tiled Algorithm for Parallel Sparse General Matrix-Matrix Multiplication on GPUs](https://ppopp22.sigplan.org/program/program-PPoPP-2022/#)**

[Yuyao Niu](https://ppopp22.sigplan.org/profile/yuyaoniu) China University of Petroleum-Beijing, [Zhengyang Lu](https://ppopp22.sigplan.org/profile/zhengyanglu) China University of Petroleum-Beijing, [Haonan Ji](https://ppopp22.sigplan.org/profile/haonanji) China University of Petroleum-Beijing, [Shuhui Song](https://ppopp22.sigplan.org/profile/shuhuisong) China University of Petroleum-Beijing, [Zhou Jin](https://ppopp22.sigplan.org/profile/zhoujin) China University of Petroleum-Beijing, [Weifeng Liu](https://ppopp22.sigplan.org/profile/weifengliu) China University of Petroleum-Beijing



**[QGTC: Accelerating Quantized Graph Neural Networks via GPU Tensor Core](https://ppopp22.sigplan.org/program/program-PPoPP-2022/#)**

[Yuke Wang](https://ppopp22.sigplan.org/profile/yukewang) UC Santa Barbara, [Boyuan Feng](https://ppopp22.sigplan.org/profile/boyuanfeng) University of California Santa Barbara, [Yufei Ding](https://ppopp22.sigplan.org/profile/yufeiding) University of California at Santa Barbara



**[FasterMoE: Modeling and Optimizing Training of Large-Scale Dynamic Pre-Trained Models](https://ppopp22.sigplan.org/program/program-PPoPP-2022/#)**

[Jiaao He](https://ppopp22.sigplan.org/profile/jiaaohe) Tsinghua University, China, [Jidong Zhai](https://ppopp22.sigplan.org/profile/jidongzhai) Tsinghua University, [Tiago Antunes](https://ppopp22.sigplan.org/profile/tiagoantunes) Tsinghua University, [Haojie Wang](https://ppopp22.sigplan.org/profile/haojiewang) Tsinghua University, [Fuwen Luo](https://ppopp22.sigplan.org/profile/fuwenluo) Tsinghua University, [Shangfeng Shi](https://ppopp22.sigplan.org/profile/shangfengshi) Tsinghua University, [Qin Li](https://ppopp22.sigplan.org/profile/qinli) Tsinghua University



**[Near-Optimal Sparse Allreduce for Distributed Deep Learning](https://ppopp22.sigplan.org/program/program-PPoPP-2022/#)**

[Shigang Li](https://ppopp22.sigplan.org/profile/shigangli) ETH Zurich, [Torsten Hoefler](https://ppopp22.sigplan.org/profile/torstenhoefler) ETH Zurich



**[BAGUALU: Targeting Brain Scale Pretrained Models with over 37 Million Cores](https://ppopp22.sigplan.org/program/program-PPoPP-2022/#)**

[Zixuan Ma](https://ppopp22.sigplan.org/profile/zixuanma) Tsinghua University, [Jiaao He](https://ppopp22.sigplan.org/profile/jiaaohe) Tsinghua University, China, [Jiezhong Qiu](https://ppopp22.sigplan.org/profile/jiezhongqiu) Tsinghua University and Beijing Academy of Artificial Intelligence, [Huanqi Cao](https://ppopp22.sigplan.org/profile/huanqicao) Tsinghua University, [Yuanwei Wang](https://ppopp22.sigplan.org/profile/yuanweiwang) Tsinghua University, [Zhenbo Sun](https://ppopp22.sigplan.org/profile/zhenbosun) Tsinghua University, [Liyan Zheng](https://ppopp22.sigplan.org/profile/liyanzheng1) Tsinghua University, [Haojie Wang](https://ppopp22.sigplan.org/profile/haojiewang) Tsinghua University, [Shizhi Tang](https://ppopp22.sigplan.org/profile/shizhitang) Tsinghua University, [Tianyu Zheng](https://ppopp22.sigplan.org/profile/tianyuzheng) Zhejiang Lab, [Junyang Lin](https://ppopp22.sigplan.org/profile/junyanglin) DAMO Academy, Alibaba Group, [Guanyu Feng](https://ppopp22.sigplan.org/profile/guanyufeng) Tsinghua University, [Zeqiang Huang](https://ppopp22.sigplan.org/profile/zeqianghuang) Zhejiang Lab, [Jie Gao](https://ppopp22.sigplan.org/profile/jiegao) Zhejiang Lab, [Aohan Zeng](https://ppopp22.sigplan.org/profile/aohanzeng) Tsinghua University and Beijing Academy of Artificial Intelligence, [Jianwei Zhang](https://ppopp22.sigplan.org/profile/jianweizhang) DAMO Academy, Alibaba Group, [Runxin Zhong](https://ppopp22.sigplan.org/profile/runxinzhong) Tsinghua University, [Tianhui Shi](https://ppopp22.sigplan.org/profile/tianhuishi) Tsinghua University, [Sha Liu](https://ppopp22.sigplan.org/profile/shaliu) Zhejiang Lab, [Weimin Zheng](https://ppopp22.sigplan.org/profile/weiminzheng) Tsinghua University, [Jie Tang](https://ppopp22.sigplan.org/profile/jietang) Tsinghua University and Beijing Academy of Artificial Intelligence, [Hongxia Yang](https://ppopp22.sigplan.org/profile/hongxiayang) DAMO Academy, Alibaba Group, [Xin Liu](https://ppopp22.sigplan.org/profile/xinliu1) Zhejiang Lab, [Jidong Zhai](https://ppopp22.sigplan.org/profile/jidongzhai) Tsinghua University, [Wenguang Chen](https://ppopp22.sigplan.org/profile/wenguangchen) Tsinghua University



### 2023

**[WISE: Predicting the Performance of Sparse Matrix Vector Multiplication with Machine Learning](https://conf.researchr.org/track/PPoPP-2023/PPoPP-2023-papers#)**

[Serif Yesil](https://ppopp23.sigplan.org/profile/serifyesil) University of Illinois Urbana-Champaign, [Azin Heidarshenas](https://ppopp23.sigplan.org/profile/azinheidarshenas) University of Illinois Urbana-Champaign, [Adam Morrison](https://ppopp23.sigplan.org/profile/adammorrison1) Tel Aviv University, [Josep Torrellas](https://ppopp23.sigplan.org/profile/joseptorrellas) University of Illinois at Urbana-Champaign



**[TGOpt: Redundancy-Aware Optimizations for Temporal Graph Attention Networks](https://conf.researchr.org/track/PPoPP-2023/PPoPP-2023-papers#)**

[Yufeng Wang](https://ppopp23.sigplan.org/profile/yufengwang) University of Illinois at Urbana-Champaign, [Charith Mendis](https://ppopp23.sigplan.org/profile/charithmendis1) University of Illinois at Urbana-Champaign

**[Dynamic N:M Fine-grained Structured Sparse Attention Mechanism](https://conf.researchr.org/track/PPoPP-2023/PPoPP-2023-papers#)**

[Zhaodong Chen](https://ppopp23.sigplan.org/profile/zhaodongchen) University of California, Santa Barbara, [Zheng Qu](https://ppopp23.sigplan.org/profile/zhengqu) University of California, Santa Barbara, [Yuying Quan](https://ppopp23.sigplan.org/profile/yuyingquan) University of California, Santa Barbara, [Liu Liu](https://ppopp23.sigplan.org/profile/liuliu2) , [Yufei Ding](https://ppopp23.sigplan.org/profile/yufeiding1) UC Santa Barbara, [Yuan Xie](https://ppopp23.sigplan.org/profile/yuanxie1) UCSB



**[Elastic Averaging for Efficient Pipelined DNN Training](https://conf.researchr.org/track/PPoPP-2023/PPoPP-2023-papers#)**[Zihao Chen](https://ppopp23.sigplan.org/profile/zihaochen) East China Normal University, [Chen Xu](https://ppopp23.sigplan.org/profile/chenxu1) East China Normal University, [Weining Qian](https://ppopp23.sigplan.org/profile/weiningqian) East China Normal University, [Aoying Zhou](https://ppopp23.sigplan.org/profile/aoyingzhou) East China Normal University



**[DSP: Efficient GNN Training with Multiple GPUs](https://conf.researchr.org/track/PPoPP-2023/PPoPP-2023-papers#)**

[Zhenkun Cai](https://ppopp23.sigplan.org/profile/zhenkuncai) The Chinese University of Hong Kong, [Qihui Zhou](https://ppopp23.sigplan.org/profile/qihuizhou) The Chinese University of Hong Kong, [Xiao Yan](https://ppopp23.sigplan.org/profile/xiaoyan) Southern University of Science and Technology, [Da Zheng](https://ppopp23.sigplan.org/profile/dazheng) Amazon Web Services, [Xiang Song](https://ppopp23.sigplan.org/profile/xiangsong) Amazon Web Services, [Chenguang Zheng](https://ppopp23.sigplan.org/profile/chenguangzheng) The Chinese University of Hong Kong, [James Cheng](https://ppopp23.sigplan.org/profile/jamescheng) The Chinese University of Hong Kong, [George Karypis](https://ppopp23.sigplan.org/profile/georgekarypis) Amazon Web Services



**[PiPAD: Pipelined and Parallel Dynamic GNN Training on GPUs](https://conf.researchr.org/track/PPoPP-2023/PPoPP-2023-papers#)**[Chunyang Wang](https://ppopp23.sigplan.org/profile/chunyangwang) Beihang University, [Desen Sun](https://ppopp23.sigplan.org/profile/desensun) Beihang University, [Yuebin Bai](https://ppopp23.sigplan.org/profile/yuebinbai) Beihang University



## MobiSys

### 2022

mGEMM: Low-latency Convolution with Minimal Memory Overhead Optimized for Mobile Devices
Jongseok Park, Kyungmin Bin, Kyunghan Lee (Seoul National University)

Band: Coordinated Multi-DNN Inference on Heterogeneous Mobile Processors
Joo Seong Jeong, Jingyu Lee, Donghyun Kim, Changmin Jeon, Changjin Jeong, Youngki Lee (Seoul National University); Byung-Gon Chun (Seoul National University, FriendliAI)

CoDL: Efficient CPU-GPU Co-execution for Deep Learning Inference on Mobile Devices
Fucheng Jia, Deyu Zhang (Central South University); Ting Cao, Shiqi Jiang (Microsoft Research); Yunxin Liu, Ju Ren, Yaoxue Zhang (Tsinghua University)

FedBalancer: Data and Pace Control for Efficient Federated Learning on Heterogeneous Clients
Jaemin Shin (School of Computing, KAIST); Yuanchun Li, Yunxin Liu (Institute for AI Industry Research (AIR), Tsinghua University); Sung-Ju Lee (School of Electrical Engineering, KAIST)

Memory-efficient DNN Training on Mobile Devices
In Gim, JeongGil Ko(Yonsei University)

Melon: Breaking the Memory Wall for Resource-Efficient On-Device Machine Learning
Qipeng Wang (Peking University); Mengwei Xu (Beijing University of Posts and Telecommunications); Chao Jin, Xinran Dong (Peking University); Jinliang Yuan (Beijing University of Posts and Telecommunications); Xin Jin, Gang Huang (Peking University); Yunxin Liu (Institute for AI Industry Research (AIR), Tsinghua University); Xuanzhe Liu (Peking University)



## ISCA

### 2020

MLPerf Inference Benchmark.

[Vijay Janapa Reddi](https://arxiv.org/search/cs?searchtype=author&query=Reddi%2C+V+J), [Christine Cheng](https://arxiv.org/search/cs?searchtype=author&query=Cheng%2C+C), [David Kanter](https://arxiv.org/search/cs?searchtype=author&query=Kanter%2C+D), [Peter Mattson](https://arxiv.org/search/cs?searchtype=author&query=Mattson%2C+P), [Guenther Schmuelling](https://arxiv.org/search/cs?searchtype=author&query=Schmuelling%2C+G), [Carole-Jean Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+C), [Brian Anderson](https://arxiv.org/search/cs?searchtype=author&query=Anderson%2C+B), [Maximilien Breughe](https://arxiv.org/search/cs?searchtype=author&query=Breughe%2C+M), [Mark Charlebois](https://arxiv.org/search/cs?searchtype=author&query=Charlebois%2C+M), [William Chou](https://arxiv.org/search/cs?searchtype=author&query=Chou%2C+W), [Ramesh Chukka](https://arxiv.org/search/cs?searchtype=author&query=Chukka%2C+R), [Cody Coleman](https://arxiv.org/search/cs?searchtype=author&query=Coleman%2C+C), [Sam Davis](https://arxiv.org/search/cs?searchtype=author&query=Davis%2C+S), [Pan Deng](https://arxiv.org/search/cs?searchtype=author&query=Deng%2C+P), [Greg Diamos](https://arxiv.org/search/cs?searchtype=author&query=Diamos%2C+G), [Jared Duke](https://arxiv.org/search/cs?searchtype=author&query=Duke%2C+J), [Dave Fick](https://arxiv.org/search/cs?searchtype=author&query=Fick%2C+D), [J. Scott Gardner](https://arxiv.org/search/cs?searchtype=author&query=Gardner%2C+J+S), [Itay Hubara](https://arxiv.org/search/cs?searchtype=author&query=Hubara%2C+I), [Sachin Idgunji](https://arxiv.org/search/cs?searchtype=author&query=Idgunji%2C+S), [Thomas B. Jablin](https://arxiv.org/search/cs?searchtype=author&query=Jablin%2C+T+B), [Jeff Jiao](https://arxiv.org/search/cs?searchtype=author&query=Jiao%2C+J), [Tom St. John](https://arxiv.org/search/cs?searchtype=author&query=John%2C+T+S), [Pankaj Kanwar](https://arxiv.org/search/cs?searchtype=author&query=Kanwar%2C+P), [David Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee%2C+D), [Jeffery Liao](https://arxiv.org/search/cs?searchtype=author&query=Liao%2C+J), [Anton Lokhmotov](https://arxiv.org/search/cs?searchtype=author&query=Lokhmotov%2C+A), [Francisco Massa](https://arxiv.org/search/cs?searchtype=author&query=Massa%2C+F), [Peng Meng](https://arxiv.org/search/cs?searchtype=author&query=Meng%2C+P), [Paulius Micikevicius](https://arxiv.org/search/cs?searchtype=author&query=Micikevicius%2C+P), [Colin Osborne](https://arxiv.org/search/cs?searchtype=author&query=Osborne%2C+C), [Gennady Pekhimenko](https://arxiv.org/search/cs?searchtype=author&query=Pekhimenko%2C+G), [Arun Tejusve Raghunath Rajan](https://arxiv.org/search/cs?searchtype=author&query=Rajan%2C+A+T+R), [Dilip Sequeira](https://arxiv.org/search/cs?searchtype=author&query=Sequeira%2C+D), [Ashish Sirasao](https://arxiv.org/search/cs?searchtype=author&query=Sirasao%2C+A), [Fei Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun%2C+F), [Hanlin Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang%2C+H), [Michael Thomson](https://arxiv.org/search/cs?searchtype=author&query=Thomson%2C+M), [Frank Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei%2C+F), [Ephrem Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu%2C+E), [Lingjie Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu%2C+L), [Koichi Yamada](https://arxiv.org/search/cs?searchtype=author&query=Yamada%2C+K), [Bing Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu%2C+B), [George Yuan](https://arxiv.org/search/cs?searchtype=author&query=Yuan%2C+G), [Aaron Zhong](https://arxiv.org/search/cs?searchtype=author&query=Zhong%2C+A), [Peizhao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang%2C+P), Yuchen Zhou



## Arxiv

### 2019

Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism.

[Mohammad Shoeybi](https://arxiv.org/search/cs?searchtype=author&query=Shoeybi%2C+M), [Mostofa Patwary](https://arxiv.org/search/cs?searchtype=author&query=Patwary%2C+M), [Raul Puri](https://arxiv.org/search/cs?searchtype=author&query=Puri%2C+R), [Patrick LeGresley](https://arxiv.org/search/cs?searchtype=author&query=LeGresley%2C+P), [Jared Casper](https://arxiv.org/search/cs?searchtype=author&query=Casper%2C+J), Bryan Catanzaro
